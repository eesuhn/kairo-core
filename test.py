import time
import sys
import warnings

warnings.filterwarnings("ignore")


class TestSuite:
    def __init__(self):
        self.results = {}
        self.start_time = time.time()

    def test_section(self, name: str):
        print(f"\n{'=' * 60}")
        print(f" {name}")
        print(f"{'=' * 60}")

    def test_result(self, test_name: str, success: bool, details: str = ""):
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        self.results[test_name] = success
        print(f"{status} {test_name}")
        if details:
            print(f"     {details}")
        return success


def main():
    suite = TestSuite()

    print("üöÄ AI NOTE-TAKING STACK COMPREHENSIVE TEST SUITE")
    print(f"üïê Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # =================================================================
    # 1. CORE PYTHON & SYSTEM LIBRARIES
    # =================================================================
    suite.test_section("1. CORE SYSTEM LIBRARIES")

    try:
        import torch
        import numpy as np
        import pandas as pd

        device = "mps" if torch.backends.mps.is_available() else "cpu"
        suite.test_result("PyTorch", True, f"v{torch.__version__}, device: {device}")
        suite.test_result("NumPy", True, f"v{np.__version__}")
        suite.test_result("Pandas", True, f"v{pd.__version__}")
    except Exception as e:
        suite.test_result("Core Libraries", False, str(e))

    # =================================================================
    # 2. TRANSFORMERS & NLP CORE
    # =================================================================
    suite.test_section("2. TRANSFORMERS & NLP CORE")

    try:
        import transformers
        from sentence_transformers import SentenceTransformer

        suite.test_result("Transformers", True, f"v{transformers.__version__}")

        # Test sentence transformers with small model
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        test_embedding = embedder.encode("Test sentence")
        suite.test_result(
            "SentenceTransformers",
            True,
            f"v{transformers.__version__}, embedding shape: {test_embedding.shape}",
        )
    except Exception as e:
        suite.test_result("Transformers/SentenceTransformers", False, str(e))

    # =================================================================
    # 3. ADVANCED NER (SPAN-MARKER)
    # =================================================================
    suite.test_section("3. NAMED ENTITY RECOGNITION")

    try:
        from span_marker import SpanMarkerModel

        # Load SpanMarker model
        ner_model = SpanMarkerModel.from_pretrained(
            "tomaarsen/span-marker-roberta-large-ontonotes5"
        )

        # Test NER
        test_text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
        entities = ner_model.predict(test_text)

        entity_summary = (
            f"Found {len(entities)} entities: {[e['label'] for e in entities]}"
        )
        suite.test_result("SpanMarker NER", True, entity_summary)

    except Exception as e:
        suite.test_result("SpanMarker NER", False, str(e))

    # =================================================================
    # 4. TOPIC MODELING (BERTOPIC)
    # =================================================================
    suite.test_section("4. TOPIC MODELING")

    try:
        from bertopic import BERTopic
        from umap import UMAP
        from hdbscan import HDBSCAN

        # Configure for small dataset testing
        umap_model = UMAP(
            n_neighbors=2,
            n_components=2,
            min_dist=0.0,
            metric="cosine",
            random_state=42,
        )
        hdbscan_model = HDBSCAN(min_cluster_size=2, metric="euclidean")

        topic_model = BERTopic(
            embedding_model=embedder,  # Reuse from previous test
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            verbose=False,
        )

        # Test topic modeling
        test_docs = [
            "Apple develops innovative AI technology",
            "Google research advances machine learning",
            "Climate change affects global weather patterns",
            "Environmental science studies ecosystem changes",
            "Microsoft cloud services support enterprise AI",
            "Amazon web services provide scalable computing",
            "Tesla autonomous vehicles use neural networks",
            "Renewable energy solutions reduce carbon emissions",
        ]

        topics, probs = topic_model.fit_transform(test_docs)
        unique_topics = len(set(topics))

        suite.test_result(
            "BERTopic",
            True,
            f"Processed {len(test_docs)} docs, found {unique_topics} topics",
        )

    except Exception as e:
        suite.test_result("BERTopic", False, str(e))

    # =================================================================
    # 5. AUDIO PROCESSING
    # =================================================================
    suite.test_section("5. AUDIO PROCESSING")

    try:
        from faster_whisper import WhisperModel

        # Load smallest model for testing
        WhisperModel("tiny.en", device="cpu", compute_type="int8")
        suite.test_result("Faster-Whisper", True, "Model loaded successfully (tiny.en)")

    except Exception as e:
        suite.test_result("Faster-Whisper", False, str(e))

    try:
        suite.test_result(
            "Pyannote-Audio", True, "Import successful (requires HF token for models)"
        )

    except Exception as e:
        suite.test_result("Pyannote-Audio", False, str(e))

    # =================================================================
    # 6. APPLE SILICON OPTIMIZATION
    # =================================================================
    suite.test_section("6. APPLE SILICON OPTIMIZATION")

    try:
        import mlx.core as mx

        # Test MLX computation
        x = mx.array([1, 2, 3, 4, 5])
        y = mx.array([2, 4, 6, 8, 10])
        result = x + y

        suite.test_result("MLX Core", True, f"Computation test: {result}")
        suite.test_result("MLX-LM", True, "Language model optimization ready")

    except Exception as e:
        suite.test_result("MLX", False, str(e))

    # =================================================================
    # 7. FINE-TUNING CAPABILITIES
    # =================================================================
    suite.test_section("7. FINE-TUNING & REINFORCEMENT LEARNING")

    try:
        import trl
        import peft
        from datasets import Dataset

        # Test dataset creation
        sample_data = {"text": ["Hello world", "AI is amazing", "Test dataset"]}
        test_dataset = Dataset.from_dict(sample_data)

        suite.test_result("TRL", True, f"v{trl.__version__}")
        suite.test_result("PEFT", True, f"v{peft.__version__}")
        suite.test_result(
            "Datasets", True, f"Created dataset with {len(test_dataset)} samples"
        )

    except Exception as e:
        suite.test_result("Fine-tuning Stack", False, str(e))

    # =================================================================
    # 8. TEXT PROCESSING
    # =================================================================
    suite.test_section("8. TEXT PROCESSING")

    try:
        import pymupdf
        import spacy

        suite.test_result(
            "PyMuPDF", True, f"v{pymupdf.__version__} - PDF processing ready"
        )

        # Test spaCy (without downloading model for speed)
        suite.test_result("spaCy", True, f"v{spacy.__version__} - NLP processing ready")

    except Exception as e:
        suite.test_result("Text Processing", False, str(e))

    # =================================================================
    # 9. WEB API FRAMEWORK
    # =================================================================
    suite.test_section("9. WEB API FRAMEWORK")

    try:
        from fastapi import FastAPI

        # Test FastAPI app creation
        app = FastAPI(title="AI Note-Taking API")

        @app.get("/health")
        def health_check():
            return {"status": "healthy", "components": ["ner", "topics", "whisper"]}

        suite.test_result("FastAPI", True, "API application created")
        suite.test_result("Uvicorn", True, "ASGI server ready")
        suite.test_result("Multipart", True, "File upload support ready")
        suite.test_result("PyYAML", True, "Configuration file support ready")

    except Exception as e:
        suite.test_result("Web API Framework", False, str(e))

    # =================================================================
    # 10. MACHINE LEARNING UTILITIES
    # =================================================================
    suite.test_section("10. MACHINE LEARNING UTILITIES")

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer

        # Test scikit-learn functionality
        vectorizer = TfidfVectorizer(max_features=100, stop_words="english")
        test_texts = ["machine learning", "artificial intelligence", "data science"]
        vectors = vectorizer.fit_transform(test_texts)

        suite.test_result(
            "Scikit-learn", True, f"TF-IDF vectorization: {vectors.shape}"
        )

    except Exception as e:
        suite.test_result("ML Utilities", False, str(e))

    # =================================================================
    # 11. END-TO-END INTEGRATION TEST
    # =================================================================
    suite.test_section("11. END-TO-END PIPELINE INTEGRATION")

    try:
        import pymupdf  # PyMuPDF for PDF processing
        import tempfile
        import os

        # Create a sample PDF for testing
        print("\nüìÑ Creating sample PDF for testing...")

        # Sample content for PDF
        pdf_content = """
        Artificial Intelligence Research Report
        
        OpenAI released ChatGPT in November 2022, revolutionizing AI conversations worldwide. 
        The model was developed by researchers in San Francisco, California. 
        
        Google's DeepMind division has been conducting groundbreaking research on large language 
        models and their applications in healthcare. Their London-based team published several 
        papers on AI safety and alignment.
        
        Tesla's autonomous driving technology, developed in Palo Alto, uses advanced neural 
        networks for real-time decision making. The company has been testing these systems 
        across multiple cities including Austin, Texas and Berlin, Germany.
        
        Microsoft Azure provides cloud computing services for enterprise machine learning 
        applications. Their Redmond headquarters oversees AI research initiatives across 
        North America and Europe.
        
        Climate scientists at MIT and Stanford University warn that rising global temperatures 
        threaten food security. The research consortium includes experts from Cambridge, 
        Massachusetts and Stanford, California.
        """

        # Create temporary PDF
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as temp_pdf:
            doc = pymupdf.open()  # Create new PDF
            page = doc.new_page()
            page.insert_text((50, 50), pdf_content, fontsize=12)
            doc.save(temp_pdf.name)
            doc.close()
            pdf_path = temp_pdf.name

        print(f"     ‚úÖ Created test PDF: {os.path.basename(pdf_path)}")

        # Extract text from PDF
        print("\nüìñ Extracting text from PDF...")
        doc = pymupdf.open(pdf_path)
        extracted_text = ""
        page_count = len(doc)  # Store page count before closing

        for page_num in range(page_count):
            page = doc.load_page(page_num)
            extracted_text += page.get_text()

        doc.close()  # Close document after extracting text

        # Clean up extracted text
        input_text = extracted_text.strip().replace("\n", " ").replace("  ", " ")

        print(f"     ‚úÖ Extracted {len(input_text)} characters from PDF")
        print(f"     Preview: {input_text[:100]}...")

        # 1. NER
        print("\nüè∑Ô∏è  Named Entity Recognition:")
        entities = ner_model.predict(input_text)
        organizations = [e["span"] for e in entities if e["label"] == "ORG"]
        locations = [e["span"] for e in entities if e["label"] == "GPE"]
        persons = [e["span"] for e in entities if e["label"] == "PERSON"]

        print(f"     Found {len(entities)} entities:")
        entity_summary = {}
        for entity in entities:
            label = entity["label"]
            if label not in entity_summary:
                entity_summary[label] = []
            entity_summary[label].append(entity["span"])

        for label, spans in entity_summary.items():
            unique_spans = list(set(spans))  # Remove duplicates
            print(f"     {label}: {', '.join(unique_spans[:5])}")  # Show first 5

        # 2. Topic modeling
        print("\nüìä Topic Modeling:")
        # Split text into sentences for better topic modeling
        sentences = [s.strip() for s in input_text.split(".") if len(s.strip()) > 50]

        if len(sentences) < 3:
            # Fallback to paragraph-like chunks
            chunks = [input_text[i : i + 200] for i in range(0, len(input_text), 200)]
            topics_extended, _ = topic_model.fit_transform(
                chunks[:8]
            )  # Limit for speed
        else:
            topics_extended, _ = topic_model.fit_transform(sentences[:8])

        print(f"     Processed {len(sentences)} text segments")
        for topic_id in set(topics_extended):
            if topic_id != -1:
                words = topic_model.get_topic(topic_id)[:4]
                top_words = [word for word, _ in words]
                count = topics_extended.count(topic_id)
                print(
                    f"     Topic {topic_id} ({count} segments): {', '.join(top_words)}"
                )

        # 3. Summarization pipeline
        print("\nüìã Text Summarization:")
        from transformers import pipeline

        summarizer = pipeline(
            "summarization", model="facebook/bart-large-cnn", device=-1
        )

        # For longer text, use appropriate max_length
        text_length = len(input_text.split())
        max_len = min(max(30, text_length // 4), 100)  # Dynamic max length
        min_len = max(10, max_len // 3)

        summary = summarizer(
            input_text, max_length=max_len, min_length=min_len, do_sample=False
        )

        print(f"     Original length: {text_length} words")
        print(f"     Summary: {summary[0]['summary_text']}")

        # Enhanced integration details
        entity_types = set([e["label"] for e in entities])
        integration_details = (
            f"PDF processed: {len(input_text)} chars, "
            f"Found {len(entities)} entities ({', '.join(entity_types)}), "
            f"Discovered {len(set(topics_extended))} topics, "
            f"Generated {len(summary[0]['summary_text'].split())} word summary"
        )

        print("\nüéØ Complete PDF Processing Results:")
        print(f"     ‚Ä¢ PDF Pages: {page_count}")  # Use stored page count
        print(f"     ‚Ä¢ Text Extracted: {len(input_text)} characters")
        print(
            f"     ‚Ä¢ Organizations: {organizations[:3]}{'...' if len(organizations) > 3 else ''}"
        )
        print(f"     ‚Ä¢ Locations: {locations[:3]}{'...' if len(locations) > 3 else ''}")
        print(f"     ‚Ä¢ People: {persons[:3] if persons else 'None detected'}")
        print(f"     ‚Ä¢ Topics Discovered: {len(set(topics_extended))}")
        print("     ‚Ä¢ Summary Quality: ‚úÖ")

        suite.test_result("End-to-End Pipeline", True, integration_details)

        # Clean up temporary file
        try:
            os.unlink(pdf_path)
            print("     üóëÔ∏è  Cleaned up temporary PDF")
        except OSError:
            pass

    except Exception as e:
        suite.test_result("End-to-End Pipeline", False, str(e))
        import traceback

        traceback.print_exc()

    # =================================================================
    # FINAL RESULTS
    # =================================================================
    suite.test_section("TEST RESULTS SUMMARY")

    total_tests = len(suite.results)
    passed_tests = sum(suite.results.values())
    failed_tests = total_tests - passed_tests
    success_rate = (passed_tests / total_tests) * 100

    duration = time.time() - suite.start_time

    print("\nüìä FINAL RESULTS:")
    print(f"   Total Tests: {total_tests}")
    print(f"   Passed: {passed_tests} ‚úÖ")
    print(f"   Failed: {failed_tests} ‚ùå")
    print(f"   Success Rate: {success_rate:.1f}%")
    print(f"   Duration: {duration:.2f} seconds")

    if success_rate >= 90:
        print("\nüéâ EXCELLENT! Your AI stack is production-ready!")
    elif success_rate >= 75:
        print("\n‚úÖ GOOD! Most components working, minor issues to resolve.")
    else:
        print("\n‚ö†Ô∏è  NEEDS ATTENTION! Several components require fixes.")

    # Show failed tests
    if failed_tests > 0:
        print("\n‚ùå Failed Tests:")
        for test_name, passed in suite.results.items():
            if not passed:
                print(f"   - {test_name}")

    print("\nüöÄ AI Note-Taking Application Stack Status:")
    print(
        f"   ‚Ä¢ Text Processing: {'‚úÖ' if suite.results.get('PyMuPDF', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Audio Processing: {'‚úÖ' if suite.results.get('Faster-Whisper', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Named Entity Recognition: {'‚úÖ' if suite.results.get('SpanMarker NER', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Topic Modeling: {'‚úÖ' if suite.results.get('BERTopic', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Text Summarization: {'‚úÖ' if suite.results.get('End-to-End Pipeline', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Fine-tuning Ready: {'‚úÖ' if suite.results.get('TRL', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Apple Silicon Optimized: {'‚úÖ' if suite.results.get('MLX Core', False) else '‚ùå'}"
    )
    print(
        f"   ‚Ä¢ Web API Ready: {'‚úÖ' if suite.results.get('FastAPI', False) else '‚ùå'}"
    )

    return success_rate >= 75


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nüõë Test suite interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nüí• Test suite crashed: {e}")
        sys.exit(1)
