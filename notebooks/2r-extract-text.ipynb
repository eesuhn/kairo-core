{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Text Extraction and Analysis\n",
    "\n",
    "This notebook processes text-based documents to:\n",
    "\n",
    "1. Extract text content from various document formats (PDF, DOCX, etc.)\n",
    "\n",
    "2. Clean and preprocess extracted text\n",
    "\n",
    "3. Perform comprehensive text analysis and visualization\n",
    "\n",
    "4. Generate structured output for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import justsdk\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "# import docx\n",
    "import _root as _  # noqa: F401\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "from configs._constants import SAMPLE_DATA_DIR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"target_file\": \"agile-method.pdf\",\n",
    "    \"target_directory\": \"text\",\n",
    "    \"supported_formats\": [\".pdf\", \".docx\", \".txt\"],\n",
    "    \"text_processing\": {\n",
    "        \"remove_extra_whitespace\": True,\n",
    "        \"preserve_paragraphs\": True,\n",
    "        \"min_word_length\": 2,\n",
    "        \"remove_numbers\": False,\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"top_words_count\": 20,\n",
    "        \"wordcloud_max_words\": 100,\n",
    "        \"sentence_sample_size\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Document Discovery and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_documents() -> Dict[str, Dict]:\n",
    "    target_path = SAMPLE_DATA_DIR / CONFIG[\"target_directory\"]\n",
    "    if not target_path.exists():\n",
    "        justsdk.print_error(f\"Target directory not found: {target_path}\")\n",
    "        return {}\n",
    "\n",
    "    documents = {}\n",
    "    for ext in CONFIG[\"supported_formats\"]:\n",
    "        for file_path in target_path.glob(f\"*{ext}\"):\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            documents[file_path.name] = {\n",
    "                \"path\": file_path,\n",
    "                \"size_mb\": round(size_mb, 3),\n",
    "                \"extension\": ext,\n",
    "                \"size_bytes\": file_path.stat().st_size,\n",
    "            }\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "available_docs = discover_documents()\n",
    "justsdk.print_info(\"Available documents:\")\n",
    "for name, info in available_docs.items():\n",
    "    print(f\"  {name} ({info['size_mb']} MB) - {info['extension'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Extraction Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentExtractor:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "\n",
    "    def extract(self, file_path: Path) -> Optional[Dict]:\n",
    "        if not file_path.exists():\n",
    "            justsdk.print_error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        extension = file_path.suffix.lower()\n",
    "\n",
    "        try:\n",
    "            if extension == \".pdf\":\n",
    "                return self._extract_pdf(file_path)\n",
    "            # elif extension == \".docx\":\n",
    "            #     return self._extract_docx(file_path)\n",
    "            elif extension == \".txt\":\n",
    "                return self._extract_txt(file_path)\n",
    "            else:\n",
    "                justsdk.print_error(f\"Unsupported file format: {extension}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            justsdk.print_error(f\"Extraction failed for {file_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_pdf(self, file_path: Path) -> Dict:\n",
    "        doc = pymupdf.open(file_path)\n",
    "        pages_text = []\n",
    "        metadata = {}\n",
    "\n",
    "        try:\n",
    "            metadata = {\n",
    "                \"title\": doc.metadata.get(\"title\", \"Unknown\"),\n",
    "                \"author\": doc.metadata.get(\"author\", \"Unknown\"),\n",
    "                \"subject\": doc.metadata.get(\"subject\", \"Unknown\"),\n",
    "                \"creator\": doc.metadata.get(\"creator\", \"Unknown\"),\n",
    "                \"page_count\": len(doc),\n",
    "            }\n",
    "        except Exception:\n",
    "            metadata = {\"page_count\": len(doc)}\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            pages_text.append(\n",
    "                {\n",
    "                    \"page_number\": page_num + 1,\n",
    "                    \"text\": page_text,\n",
    "                    \"char_count\": len(page_text),\n",
    "                    \"word_count\": len(page_text.split()),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        raw_text = \"\\n\".join([page[\"text\"] for page in pages_text])\n",
    "\n",
    "        return {\n",
    "            \"file_name\": file_path.name,\n",
    "            \"file_type\": \"PDF\",\n",
    "            \"metadata\": metadata,\n",
    "            \"pages\": pages_text,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"processed_text\": self._process_text(raw_text),\n",
    "        }\n",
    "\n",
    "    # def _extract_docx(self, file_path: Path) -> Dict:\n",
    "    #     doc = docx.Document(file_path)\n",
    "    #     paragraphs = []\n",
    "\n",
    "    #     for i, para in enumerate(doc.paragraphs):\n",
    "    #         paragraphs.append(\n",
    "    #             {\n",
    "    #                 \"paragraph_number\": i + 1,\n",
    "    #                 \"text\": para.text,\n",
    "    #                 \"char_count\": len(para.text),\n",
    "    #                 \"word_count\": len(para.text.split()),\n",
    "    #             }\n",
    "    #         )\n",
    "\n",
    "    #     raw_text = \"\\n\".join([para[\"text\"] for para in paragraphs])\n",
    "\n",
    "    #     return {\n",
    "    #         \"file_name\": file_path.name,\n",
    "    #         \"file_type\": \"DOCX\",\n",
    "    #         \"metadata\": {\"paragraph_count\": len(paragraphs)},\n",
    "    #         \"paragraphs\": paragraphs,\n",
    "    #         \"raw_text\": raw_text,\n",
    "    #         \"processed_text\": self._process_text(raw_text),\n",
    "    #     }\n",
    "\n",
    "    def _extract_txt(self, file_path: Path) -> Dict:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        lines = raw_text.split(\"\\n\")\n",
    "        line_data = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_data.append(\n",
    "                {\n",
    "                    \"line_number\": i + 1,\n",
    "                    \"text\": line,\n",
    "                    \"char_count\": len(line),\n",
    "                    \"word_count\": len(line.split()),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"file_name\": file_path.name,\n",
    "            \"file_type\": \"TXT\",\n",
    "            \"metadata\": {\"line_count\": len(lines)},\n",
    "            \"lines\": line_data,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"processed_text\": self._process_text(raw_text),\n",
    "        }\n",
    "\n",
    "    def _process_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and process extracted text.\"\"\"\n",
    "\n",
    "        processed = text\n",
    "\n",
    "        if self.config[\"text_processing\"][\"remove_extra_whitespace\"]:\n",
    "            processed = re.sub(r\"\\s+\", \" \", processed)\n",
    "\n",
    "        if not self.config[\"text_processing\"][\"preserve_paragraphs\"]:\n",
    "            processed = processed.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        else:\n",
    "            processed = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", processed)\n",
    "            processed = re.sub(r\"[ \\t]+\", \" \", processed)\n",
    "\n",
    "        return processed.strip()\n",
    "\n",
    "\n",
    "extractor = DocumentExtractor(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Analysis Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.stop_words = {\n",
    "            \"a\",\n",
    "            \"an\",\n",
    "            \"and\",\n",
    "            \"are\",\n",
    "            \"as\",\n",
    "            \"at\",\n",
    "            \"be\",\n",
    "            \"by\",\n",
    "            \"for\",\n",
    "            \"from\",\n",
    "            \"has\",\n",
    "            \"he\",\n",
    "            \"in\",\n",
    "            \"is\",\n",
    "            \"it\",\n",
    "            \"its\",\n",
    "            \"of\",\n",
    "            \"on\",\n",
    "            \"that\",\n",
    "            \"the\",\n",
    "            \"to\",\n",
    "            \"was\",\n",
    "            \"were\",\n",
    "            \"will\",\n",
    "            \"with\",\n",
    "            \"you\",\n",
    "            \"your\",\n",
    "            \"this\",\n",
    "            \"they\",\n",
    "            \"have\",\n",
    "            \"had\",\n",
    "            \"what\",\n",
    "            \"when\",\n",
    "            \"where\",\n",
    "            \"who\",\n",
    "            \"which\",\n",
    "            \"why\",\n",
    "            \"how\",\n",
    "        }\n",
    "\n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        basic_stats = self._calculate_basic_stats(text)\n",
    "        word_analysis = self._analyze_words(text)\n",
    "        sentence_analysis = self._analyze_sentences(text)\n",
    "        reading_metrics = self._calculate_reading_metrics(text)\n",
    "\n",
    "        return {\n",
    "            \"basic_stats\": basic_stats,\n",
    "            \"word_analysis\": word_analysis,\n",
    "            \"sentence_analysis\": sentence_analysis,\n",
    "            \"reading_metrics\": reading_metrics,\n",
    "        }\n",
    "\n",
    "    def _calculate_basic_stats(self, text: str) -> Dict:\n",
    "        words = text.split()\n",
    "        sentences = re.split(r\"[.!?]+\", text)\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "        return {\n",
    "            \"char_count\": len(text),\n",
    "            \"char_count_no_spaces\": len(text.replace(\" \", \"\")),\n",
    "            \"word_count\": len(words),\n",
    "            \"sentence_count\": len([s for s in sentences if s.strip()]),\n",
    "            \"paragraph_count\": len([p for p in paragraphs if p.strip()]),\n",
    "            \"avg_words_per_sentence\": len(words)\n",
    "            / max(len([s for s in sentences if s.strip()]), 1),\n",
    "            \"avg_chars_per_word\": len(text.replace(\" \", \"\")) / max(len(words), 1),\n",
    "        }\n",
    "\n",
    "    def _analyze_words(self, text: str) -> Dict:\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "        min_length = self.config[\"text_processing\"][\"min_word_length\"]\n",
    "        filtered_words = [\n",
    "            word\n",
    "            for word in words\n",
    "            if len(word) >= min_length and word not in self.stop_words\n",
    "        ]\n",
    "\n",
    "        word_freq = Counter(filtered_words)\n",
    "        word_lengths = [len(word) for word in words]\n",
    "\n",
    "        return {\n",
    "            \"unique_words\": len(set(words)),\n",
    "            \"unique_words_filtered\": len(set(filtered_words)),\n",
    "            \"most_common_words\": word_freq.most_common(\n",
    "                self.config[\"analysis\"][\"top_words_count\"]\n",
    "            ),\n",
    "            \"avg_word_length\": np.mean(word_lengths) if word_lengths else 0,\n",
    "            \"word_length_distribution\": Counter(word_lengths),\n",
    "            \"vocabulary_richness\": len(set(words)) / max(len(words), 1),\n",
    "        }\n",
    "\n",
    "    def _analyze_sentences(self, text: str) -> Dict:\n",
    "        sentences = [s.strip() for s in re.split(r\"[.!?]+\", text) if s.strip()]\n",
    "        sentence_lengths = [len(s.split()) for s in sentences]\n",
    "\n",
    "        return {\n",
    "            \"sentence_count\": len(sentences),\n",
    "            \"avg_sentence_length\": np.mean(sentence_lengths) if sentence_lengths else 0,\n",
    "            \"median_sentence_length\": np.median(sentence_lengths)\n",
    "            if sentence_lengths\n",
    "            else 0,\n",
    "            \"sentence_length_std\": np.std(sentence_lengths) if sentence_lengths else 0,\n",
    "            \"shortest_sentence\": min(sentence_lengths) if sentence_lengths else 0,\n",
    "            \"longest_sentence\": max(sentence_lengths) if sentence_lengths else 0,\n",
    "            \"sample_sentences\": sentences[\n",
    "                : self.config[\"analysis\"][\"sentence_sample_size\"]\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def _calculate_reading_metrics(self, text: str) -> Dict:\n",
    "        words = text.split()\n",
    "        sentences = [s for s in re.split(r\"[.!?]+\", text) if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return {\"reading_time_minutes\": 0, \"reading_level\": \"Unknown\"}\n",
    "\n",
    "        reading_time = len(words) / 200\n",
    "\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_word_length = np.mean([len(word) for word in words])\n",
    "\n",
    "        complexity_score = (avg_sentence_length * 0.4) + (avg_word_length * 1.5)\n",
    "\n",
    "        if complexity_score < 8:\n",
    "            reading_level = \"Elementary\"\n",
    "        elif complexity_score < 12:\n",
    "            reading_level = \"Middle School\"\n",
    "        elif complexity_score < 16:\n",
    "            reading_level = \"High School\"\n",
    "        else:\n",
    "            reading_level = \"College+\"\n",
    "\n",
    "        return {\n",
    "            \"reading_time_minutes\": round(reading_time, 1),\n",
    "            \"reading_level\": reading_level,\n",
    "            \"complexity_score\": round(complexity_score, 2),\n",
    "        }\n",
    "\n",
    "\n",
    "analyzer = TextAnalyzer(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"target_file\"] not in available_docs:\n",
    "    justsdk.print_error(f\"Target file '{CONFIG['target_file']}' not found\")\n",
    "    if available_docs:\n",
    "        CONFIG[\"target_file\"] = list(available_docs.keys())[0]\n",
    "        justsdk.print_info(f\"Using first available file: {CONFIG['target_file']}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No documents found\")\n",
    "\n",
    "target_doc = available_docs[CONFIG[\"target_file\"]]\n",
    "justsdk.print_info(\n",
    "    f\"Processing: {target_doc['path'].name} ({target_doc['size_mb']} MB)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Extracting text...\")\n",
    "extraction_result = extractor.extract(target_doc[\"path\"])\n",
    "\n",
    "if extraction_result:\n",
    "    justsdk.print_success(\"Text extraction completed\")\n",
    "    print(f\"File type: {extraction_result['file_type']}\")\n",
    "    print(f\"Raw text length: {len(extraction_result['raw_text']):,} characters\")\n",
    "    print(\n",
    "        f\"Processed text length: {len(extraction_result['processed_text']):,} characters\"\n",
    "    )\n",
    "\n",
    "    if extraction_result[\"metadata\"]:\n",
    "        print(\"\\nDocument metadata:\")\n",
    "        for key, value in extraction_result[\"metadata\"].items():\n",
    "            print(f\"  {key.title()}: {value}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Text extraction failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Analyzing text...\")\n",
    "analysis_result = analyzer.analyze(extraction_result[\"processed_text\"])\n",
    "\n",
    "justsdk.print_success(\"Text analysis completed\")\n",
    "stats = analysis_result[\"basic_stats\"]\n",
    "reading = analysis_result[\"reading_metrics\"]\n",
    "\n",
    "print(f\"\"\"\\nText Statistics:\n",
    "  Characters: {stats[\"char_count\"]:,} (including spaces)\n",
    "  Words: {stats[\"word_count\"]:,}\n",
    "  Sentences: {stats[\"sentence_count\"]:,}\n",
    "  Paragraphs: {stats[\"paragraph_count\"]:,}\n",
    "  Average words per sentence: {stats[\"avg_words_per_sentence\"]:.1f}\n",
    "  Average characters per word: {stats[\"avg_chars_per_word\"]:.1f}\n",
    "\n",
    "Reading Metrics:\n",
    "  Estimated reading time: {reading[\"reading_time_minutes\"]} minutes\n",
    "  Reading level: {reading[\"reading_level\"]}\n",
    "  Complexity score: {reading[\"complexity_score\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Text Content Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Text Content Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW TEXT (first 500 characters):\")\n",
    "print(textwrap.fill(extraction_result[\"raw_text\"][:500] + \"...\", width=80))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESSED TEXT (first 500 characters):\")\n",
    "print(textwrap.fill(extraction_result[\"processed_text\"][:500] + \"...\", width=80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_sentences = analysis_result[\"sentence_analysis\"][\"sample_sentences\"]\n",
    "if sample_sentences:\n",
    "    print(\"\\nSample sentences:\")\n",
    "    for i, sentence in enumerate(sample_sentences[:5], 1):\n",
    "        wrapped_sentence = textwrap.fill(\n",
    "            sentence, width=75, initial_indent=f\"  {i}. \", subsequent_indent=\"     \"\n",
    "        )\n",
    "        print(wrapped_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Text Analysis Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = analysis_result[\"word_analysis\"]\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "if word_data[\"most_common_words\"]:\n",
    "    words, counts = zip(*word_data[\"most_common_words\"][:15])\n",
    "    ax1.barh(range(len(words)), counts, color=\"skyblue\")\n",
    "    ax1.set_yticks(range(len(words)))\n",
    "    ax1.set_yticklabels(words)\n",
    "    ax1.set_xlabel(\"Frequency\")\n",
    "    ax1.set_title(\"Most Common Words (excluding stop words)\")\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "length_dist = word_data[\"word_length_distribution\"]\n",
    "if length_dist:\n",
    "    lengths = sorted(length_dist.keys())\n",
    "    counts = [length_dist[length] for length in lengths]\n",
    "    ax2.bar(lengths, counts, color=\"lightcoral\", alpha=0.7)\n",
    "    ax2.set_xlabel(\"Word Length (characters)\")\n",
    "    ax2.set_ylabel(\"Frequency\")\n",
    "    ax2.set_title(\"Word Length Distribution\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "sentence_data = analysis_result[\"sentence_analysis\"]\n",
    "sample_sentences = sentence_data[\"sample_sentences\"]\n",
    "sentence_lengths = (\n",
    "    [len(s.split()) for s in sample_sentences] if sample_sentences else []\n",
    ")\n",
    "\n",
    "if sentence_lengths:\n",
    "    ax3.hist(\n",
    "        sentence_lengths,\n",
    "        bins=min(10, len(set(sentence_lengths))),\n",
    "        color=\"lightgreen\",\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax3.axvline(\n",
    "        sentence_data[\"avg_sentence_length\"],\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Average: {sentence_data['avg_sentence_length']:.1f}\",\n",
    "    )\n",
    "    ax3.set_xlabel(\"Sentence Length (words)\")\n",
    "    ax3.set_ylabel(\"Frequency\")\n",
    "    ax3.set_title(\"Sentence Length Distribution (sample)\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "metrics = [\n",
    "    (\"Vocabulary\\nRichness\", word_data[\"vocabulary_richness\"]),\n",
    "    (\"Avg Word\\nLength\", word_data[\"avg_word_length\"]),\n",
    "    (\"Avg Sentence\\nLength\", sentence_data[\"avg_sentence_length\"]),\n",
    "    (\"Complexity\\nScore\", analysis_result[\"reading_metrics\"][\"complexity_score\"]),\n",
    "]\n",
    "\n",
    "metric_names, metric_values = zip(*metrics)\n",
    "colors = [\"gold\", \"lightblue\", \"lightgreen\", \"salmon\"]\n",
    "ax4.bar(metric_names, metric_values, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel(\"Score\")\n",
    "ax4.set_title(\"Text Complexity Metrics\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(metric_values):\n",
    "    ax4.text(\n",
    "        i,\n",
    "        v + max(metric_values) * 0.01,\n",
    "        f\"{v:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (_, ax2) = plt.subplots(1, 2, figsize=(16, 8))  # noqa: F811\n",
    "\n",
    "# NOTE: WordCloud is only for cosmetic, removing this dep for now\n",
    "# if word_data[\"most_common_words\"]:\n",
    "#     word_freq_dict = dict(\n",
    "#         word_data[\"most_common_words\"][: CONFIG[\"analysis\"][\"wordcloud_max_words\"]]\n",
    "#     )\n",
    "\n",
    "#     wordcloud = WordCloud(\n",
    "#         width=800,\n",
    "#         height=400,\n",
    "#         background_color=\"white\",\n",
    "#         max_words=CONFIG[\"analysis\"][\"wordcloud_max_words\"],\n",
    "#         colormap=\"viridis\",\n",
    "#     ).generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "#     ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#     ax1.axis(\"off\")\n",
    "#     ax1.set_title(\"Word Cloud (Most Frequent Words)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "stats = analysis_result[\"basic_stats\"]\n",
    "structure_data = [\n",
    "    (\"Characters\", stats[\"char_count\"]),\n",
    "    (\"Words\", stats[\"word_count\"]),\n",
    "    (\"Sentences\", stats[\"sentence_count\"]),\n",
    "    (\"Paragraphs\", stats[\"paragraph_count\"]),\n",
    "]\n",
    "\n",
    "labels, sizes = zip(*structure_data)\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "\n",
    "# Use actual sizes for pie chart display\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct=lambda pct: f\"{pct:.1f}%\",\n",
    "    startangle=90,\n",
    ")\n",
    "\n",
    "# Add actual counts as text in the center or as annotations\n",
    "for i, (label, size) in enumerate(structure_data):\n",
    "    ax2.annotate(\n",
    "        f\"{size:,}\",\n",
    "        xy=(wedges[i].theta2 - (wedges[i].theta2 - wedges[i].theta1) / 2, 0.7),\n",
    "        xycoords=\"data\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "ax2.set_title(\"Document Structure\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
