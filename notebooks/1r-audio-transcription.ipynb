{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881f97fd-b8ea-47cf-9450-7162772f74ae",
   "metadata": {},
   "source": [
    "## \\[Research\\] Audio transcription with `faster-whisper`\n",
    "\n",
    "This notebook demonstrates the audio processing pipeline, focusing on transcribing audio with `faster-whisper`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7beec-d477-4e59-bfe8-5ce5691644b8",
   "metadata": {},
   "source": [
    "### 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c7610-15a7-4e8a-95bb-28c3c460c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import justsdk\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from faster_whisper import WhisperModel\n",
    "from datetime import timedelta\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "SAMPLE_DIR = DATA_DIR / \"sample\" / \"audio\"\n",
    "MODEL_DIR = ROOT / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e827c7e",
   "metadata": {},
   "source": [
    "### 2. Setting configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL = \"base\"\n",
    "\n",
    "WHISPER_CONFIG = {\n",
    "    \"model_size\": WHISPER_MODEL,\n",
    "    \"device\": \"cpu\",\n",
    "    \"compute_type\": \"int8\",\n",
    "    \"num_workers\": 2,\n",
    "    \"download_root\": str(MODEL_DIR / f\"whisper-{WHISPER_MODEL}\"),\n",
    "}\n",
    "\n",
    "AUDIO_CONFIG = {\n",
    "    \"language\": \"en\",\n",
    "    \"task\": \"transcribe\",\n",
    "    \"beam_size\": 5,  # Paths searches during decoding\n",
    "    \"best_of\": 5,\n",
    "    \"patience\": 1,\n",
    "    \"length_penalty\": 1,\n",
    "    \"temperature\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  # Temperature fallback\n",
    "    \"compression_ratio_threshold\": 2.4,  # Reject if text is too repetitive\n",
    "    \"log_prob_threshold\": -1.0,  # Threshold for confidence levels\n",
    "    \"no_speech_threshold\": 0.6,  # Threshold for non-speech detection\n",
    "    \"word_timestamps\": True,  # Generate word-level timestamps\n",
    "    \"vad_filter\": True,  # Skip silent parts\n",
    "    \"vad_parameters\": {\n",
    "        \"threshold\": 0.5,\n",
    "        \"min_speech_duration_ms\": 250,\n",
    "        \"max_speech_duration_s\": float(\"inf\"),\n",
    "        \"min_silence_duration_ms\": 2000,\n",
    "        \"speech_pad_ms\": 400,\n",
    "    },\n",
    "}\n",
    "\n",
    "justsdk.print_info(f\"WHISPER_MODEL: {WHISPER_MODEL}\")\n",
    "justsdk.print_info(f\"DEVICE: {WHISPER_CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a951a",
   "metadata": {},
   "source": [
    "### 3. Linking sample audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51824db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = []\n",
    "audio_ext = [\".wav\", \".mp3\", \".mp4\"]\n",
    "\n",
    "for ext in audio_ext:\n",
    "    sample_audio.extend(SAMPLE_DIR.glob(f\"*{ext}\"))\n",
    "\n",
    "# Check the sample audio size in MB\n",
    "justsdk.print_info(\"sample_audio:\")\n",
    "for audio in sample_audio:\n",
    "    size_mb = audio.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {audio.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7378ad",
   "metadata": {},
   "source": [
    "### 4. Initializing `faster-whisper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d08585",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = WhisperModel(\n",
    "        WHISPER_CONFIG[\"model_size\"],\n",
    "        device=WHISPER_CONFIG[\"device\"],\n",
    "        compute_type=WHISPER_CONFIG[\"compute_type\"],\n",
    "        download_root=WHISPER_CONFIG[\"download_root\"],\n",
    "        num_workers=WHISPER_CONFIG[\"num_workers\"],\n",
    "    )\n",
    "    justsdk.print_success(\"Whisper model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    justsdk.print_error(f\"Failed to load Whisper model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf81f3",
   "metadata": {},
   "source": [
    "### 5. Functions to transcribe audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to HH:MM:SS.mmm format\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    hours = int(td.total_seconds() // 3600)\n",
    "    minutes = int((td.total_seconds() % 3600) // 60)\n",
    "    seconds = td.total_seconds() % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path: Path, model: WhisperModel, config: dict) -> dict:\n",
    "    try:\n",
    "        segments_generator, info = model.transcribe(\n",
    "            str(audio_path),\n",
    "            language=config[\"language\"],\n",
    "            task=config[\"task\"],\n",
    "            beam_size=config[\"beam_size\"],\n",
    "            best_of=config[\"best_of\"],\n",
    "            patience=config[\"patience\"],\n",
    "            length_penalty=config[\"length_penalty\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            compression_ratio_threshold=config[\"compression_ratio_threshold\"],\n",
    "            log_prob_threshold=config[\"log_prob_threshold\"],\n",
    "            no_speech_threshold=config[\"no_speech_threshold\"],\n",
    "            word_timestamps=config[\"word_timestamps\"],\n",
    "            vad_filter=config[\"vad_filter\"],\n",
    "            vad_parameters=config[\"vad_parameters\"],\n",
    "        )\n",
    "        segments = list(segments_generator)\n",
    "\n",
    "        processed_segments = []\n",
    "        word_segments = []\n",
    "        full_text = \"\"\n",
    "\n",
    "        for segment in segments:\n",
    "            seg_dict = {\n",
    "                \"id\": segment.id,\n",
    "                \"start\": segment.start,\n",
    "                \"end\": segment.end,\n",
    "                \"text\": segment.text.strip(),\n",
    "                \"tokens\": segment.tokens,\n",
    "                \"temperature\": segment.temperature,\n",
    "                \"avg_logprob\": segment.avg_logprob,\n",
    "                \"compression_ratio\": segment.compression_ratio,\n",
    "                \"no_speech_prob\": segment.no_speech_prob,\n",
    "                \"start_formatted\": format_timestamp(segment.start),\n",
    "                \"end_formatted\": format_timestamp(segment.end),\n",
    "                \"duration\": segment.end - segment.start,\n",
    "            }\n",
    "            processed_segments.append(seg_dict)\n",
    "            full_text += seg_dict[\"text\"] + \" \"\n",
    "\n",
    "            if hasattr(segment, \"words\") and segment.words:\n",
    "                for word in segment.words:\n",
    "                    word_segments.append(\n",
    "                        {\n",
    "                            \"word\": word.word,\n",
    "                            \"start\": word.start,\n",
    "                            \"end\": word.end,\n",
    "                            \"probability\": word.probability,\n",
    "                            \"start_formatted\": format_timestamp(word.start),\n",
    "                            \"end_formatted\": format_timestamp(word.end),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        audio_info = {\n",
    "            \"language\": info.language,\n",
    "            \"language_probability\": info.language_probability,\n",
    "            \"duration\": info.duration,\n",
    "            \"duration_formatted\": format_timestamp(info.duration),\n",
    "        }\n",
    "\n",
    "        result = {\n",
    "            \"file\": audio_path.name,\n",
    "            \"num_segments\": len(processed_segments),\n",
    "            \"segments\": processed_segments,\n",
    "            \"num_words\": len(word_segments),\n",
    "            \"words\": word_segments,\n",
    "            \"full_text\": full_text.strip(),\n",
    "            \"audio_info\": audio_info,\n",
    "        }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        justsdk.print_error(f\"Failed to transcribe {audio_path.name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4e0b4",
   "metadata": {},
   "source": [
    "### 6. Transcribing sample audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d21c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = None\n",
    "if sample_audio:\n",
    "    selected_audio = sample_audio[0]\n",
    "    justsdk.print_info(f\"Transcribing audio: {selected_audio.name}\")\n",
    "    display(Audio(str(selected_audio)))\n",
    "    transcription = transcribe_audio(selected_audio, model, AUDIO_CONFIG)\n",
    "else:\n",
    "    justsdk.print_error(\"No sample audio files found to transcribe.\")\n",
    "\n",
    "if transcription:\n",
    "    justsdk.print_success(f\"Transcription for {transcription['file']} completed.\")\n",
    "    print(f\"  Total segments: {transcription['num_segments']}\")\n",
    "    print(f\"  Total words: {transcription['num_words']}\")\n",
    "    print(f\"{'=' * 10} Full Text {'=' * 10}\")\n",
    "    print(transcription[\"full_text\"])\n",
    "    print(f\"{'=' * 31}\")\n",
    "    print(f\"\\n  Audio duration: {transcription['audio_info']['duration_formatted']}\")\n",
    "else:\n",
    "    justsdk.print_error(\"Transcription failed or no audio processed.\")\n",
    "\n",
    "if transcription and transcription[\"segments\"]:\n",
    "    segments_df = pd.DataFrame(transcription[\"segments\"])\n",
    "    for index, seg in segments_df.iterrows():\n",
    "        justsdk.print_info(\n",
    "            f\"  [{seg['start_formatted']} -> {seg['end_formatted']}] ({seg['duration']:.2f}s)\",\n",
    "            newline_before=True\n",
    "        )\n",
    "        print(f\"{'=' * 10} Text {'=' * 10}\")\n",
    "        print(seg[\"text\"])\n",
    "        print(f\"{'=' * 26}\")\n",
    "        print(f\"  Confidence: {seg['avg_logprob']:.4f}\")\n",
    "        print(f\"  Non speech probability: {seg['no_speech_prob']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39bd88",
   "metadata": {},
   "source": [
    "### 7. Visualizing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcription and transcription[\"segments\"]:\n",
    "    segments_df = pd.DataFrame(transcription[\"segments\"])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Segment durations\n",
    "    axes[0, 0].set_title(\"Segment Durations\")\n",
    "    axes[0, 0].bar(range(len(segments_df)), segments_df[\"duration\"])\n",
    "    axes[0, 0].set_xlabel(\"Segment ID\")\n",
    "    axes[0, 0].set_ylabel(\"Duration (s)\")\n",
    "\n",
    "    # Confidence scores\n",
    "    axes[0, 1].set_title(\n",
    "        \"Transcription Confidence by Segment (Negative Log Probability)\"\n",
    "    )\n",
    "    axes[0, 1].plot(segments_df.index, -segments_df[\"avg_logprob\"], \"o-\")\n",
    "    axes[0, 1].set_xlabel(\"Segment ID\")\n",
    "    axes[0, 1].set_ylabel(\"Confidence Score\")\n",
    "\n",
    "    # Words per segment\n",
    "    words_per_segment = segments_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "    axes[1, 0].set_title(\"Words per Segment\")\n",
    "    axes[1, 0].bar(range(len(segments_df)), words_per_segment)\n",
    "    axes[1, 0].set_xlabel(\"Segment ID\")\n",
    "    axes[1, 0].set_ylabel(\"Number of Words\")\n",
    "\n",
    "    # Non-speech probability\n",
    "    axes[1, 1].set_title(\"Non-Speech Probability by Segment\")\n",
    "    axes[1, 1].scatter(\n",
    "        segments_df.index,\n",
    "        segments_df[\"no_speech_prob\"],\n",
    "        c=segments_df[\"no_speech_prob\"],\n",
    "        cmap=\"RdYlGn_r\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Segment ID\")\n",
    "    axes[1, 1].set_ylabel(\"Non-Speech Probability\")\n",
    "    axes[1, 1].axhline(y=0.5, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Threshold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    justsdk.print_info(\"Summary statistics:\")\n",
    "    print(f\"  Average segment duration(s): {segments_df['duration'].mean():.2f}\")\n",
    "    print(f\"  Average confidence score: {-segments_df['avg_logprob'].mean():.4f}\")\n",
    "    print(f\"  Average words per segment: {words_per_segment.mean():.2f}\")\n",
    "    print(\n",
    "        f\"  Average non-speech probability: {segments_df['no_speech_prob'].mean():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Total audio duration(s): {transcription['audio_info']['duration_formatted']}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
