{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "## [Research] End-to-End NLP Pipeline Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "This notebook replicates the end-to-end pipeline to test:\n",
    "\n",
    "- PDF text extraction\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "- Topic modeling\n",
    "\n",
    "- Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba9831",
   "metadata": {},
   "source": [
    "### 1. Imports + Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import justsdk\n",
    "\n",
    "from span_marker import SpanMarkerModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "SAMPLE_DIR = DATA_DIR / \"sample\" / \"pdf\"\n",
    "SAMPLE_PATH = SAMPLE_DIR / \"01-agile-methodology.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "### 2. Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"embedder\": SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
    "    \"ner\": SpanMarkerModel.from_pretrained(\n",
    "        \"tomaarsen/span-marker-roberta-large-ontonotes5\"\n",
    "    ),\n",
    "    \"summarizer\": pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "### 3. Extract text from sample PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(SAMPLE_PATH)\n",
    "text = \"\".join([doc.load_page(p).get_text() for p in range(len(doc))])\n",
    "doc.close()\n",
    "\n",
    "text_clean = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "justsdk.print_info(f\"Extracted {len(text_clean)} characters\", newline_before=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "### 4. Perform Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = models[\"ner\"].predict(text_clean)\n",
    "orgs = [e[\"span\"] for e in entities if e[\"label\"] == \"ORG\"]\n",
    "locations = [e[\"span\"] for e in entities if e[\"label\"] == \"GPE\"]\n",
    "people = [e[\"span\"] for e in entities if e[\"label\"] == \"PERSON\"]\n",
    "\n",
    "justsdk.print_info(f\"Found {len(entities)} entities:\")\n",
    "print(f\"  Organizations: {orgs[:3]}\")\n",
    "print(f\"  Locations: {locations[:3]}\")\n",
    "print(f\"  People: {people[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "### 5. Topic modeling with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s.strip() for s in text_clean.split(\".\") if len(s.strip()) > 50]\n",
    "if len(sentences) >= 3:\n",
    "    umap_model = UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric=\"cosine\")\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=2, metric=\"euclidean\")\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=models[\"embedder\"],\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        verbose=False,\n",
    "    )\n",
    "    topics, _ = topic_model.fit_transform(sentences[:8])\n",
    "    justsdk.print_info(\n",
    "        f\"Discovered {len(set(topics))} topics from {len(sentences)} sentences\"\n",
    "    )\n",
    "else:\n",
    "    topics = []\n",
    "    justsdk.print_warning(\n",
    "        \"Not enough sentences to discover topics, skipping topic modeling.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "### 6. Summarize text in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=800) -> list[str]:\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i : i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "\n",
    "chunks = chunk_text(text_clean)\n",
    "justsdk.print_info(f\"Split document into {len(chunks)} chunks\")\n",
    "\n",
    "chunk_summaries = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    max_len = min(100, len(chunk.split()) // 3)\n",
    "    min_len = min(20, max_len // 2)\n",
    "    summary = models[\"summarizer\"](\n",
    "        chunk, max_length=max_len, min_length=min_len, do_sample=False\n",
    "    )\n",
    "    summary_text = summary[0][\"summary_text\"]\n",
    "    chunk_summaries.append(summary_text)\n",
    "    print(f\"  Chunk {i + 1}: {summary_text}\")\n",
    "\n",
    "combined_summary = \" \".join(chunk_summaries)\n",
    "if \".\" in combined_summary:\n",
    "    combined_summary = combined_summary.replace(\".\", \".\\n\")\n",
    "\n",
    "justsdk.print_success(\n",
    "    f\"Summary ({len(combined_summary.split())} words):\\n {combined_summary}\",\n",
    "    newline_before=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
