{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription and Speaker Diarization\n",
    "\n",
    "This notebook processes audio files to:\n",
    "\n",
    "1. Generate accurate transcriptions using faster-whisper\n",
    "\n",
    "2. Identify and separate speakers using pyannote-audio\n",
    "\n",
    "3. Combine results into a structured, speaker-attributed transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import justsdk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import _root as _  # noqa: F401\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "from faster_whisper import WhisperModel\n",
    "from IPython.display import Audio, display\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from config._constants import HF_READ_ONLY_TOKEN, SAMPLE_DATA_DIR, MODEL_DIR\n",
    "from src.utils import Utils\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"target_file\": \"project-proposal.mp3\",\n",
    "    \"whisper\": {\n",
    "        \"model_size\": \"base.en\",\n",
    "        \"compute_type\": \"int8\",\n",
    "        \"num_workers\": 2,\n",
    "        # \"local_files_only\": True,\n",
    "    },\n",
    "    \"transcription\": {\n",
    "        \"language\": \"en\",\n",
    "        \"word_timestamps\": True,\n",
    "        \"vad_filter\": True,\n",
    "        \"vad_parameters\": {\"min_speech_duration_ms\": 250},\n",
    "    },\n",
    "    \"diarization\": {\"overlap_threshold\": 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Audio File Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_audio_files() -> Dict[str, Dict]:\n",
    "    extensions = [\".mp3\", \".mp4\", \".wav\"]\n",
    "    directories = [\"audio\", \"video\"]\n",
    "\n",
    "    files = {}\n",
    "    for directory in directories:\n",
    "        dir_path = SAMPLE_DATA_DIR / directory\n",
    "        if dir_path.exists():\n",
    "            for ext in extensions:\n",
    "                for file_path in dir_path.glob(f\"*{ext}\"):\n",
    "                    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    files[file_path.name] = {\n",
    "                        \"path\": file_path,\n",
    "                        \"size_mb\": round(size_mb, 2),\n",
    "                    }\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "audio_files = discover_audio_files()\n",
    "justsdk.print_info(\"Available audio files:\")\n",
    "for name, info in audio_files.items():\n",
    "    print(f\"  {name} ({info['size_mb']} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transcription Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionEngine:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> Optional[WhisperModel]:\n",
    "        try:\n",
    "            model_params = {\n",
    "                \"model_size_or_path\": self.config[\"whisper\"][\"model_size\"],\n",
    "                \"compute_type\": self.config[\"whisper\"][\"compute_type\"],\n",
    "                \"num_workers\": self.config[\"whisper\"][\"num_workers\"],\n",
    "                \"download_root\": str(MODEL_DIR / \"whisper-base-en\"),\n",
    "                # \"local_files_only\": self.config[\"whisper\"][\"local_files_only\"],\n",
    "            }\n",
    "            model = WhisperModel(**model_params)\n",
    "            justsdk.print_success(\"Whisper model loaded successfully\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            justsdk.print_error(f\"Failed to load Whisper model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def transcribe(self, audio_path: Path) -> Optional[Dict]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            segments_gen, info = self.model.transcribe(\n",
    "                str(audio_path), **self.config[\"transcription\"]\n",
    "            )\n",
    "\n",
    "            segments = []\n",
    "            words = []\n",
    "            full_text = []\n",
    "\n",
    "            for segment in segments_gen:\n",
    "                seg_data = self._process_segment(segment)\n",
    "                segments.append(seg_data)\n",
    "                full_text.append(seg_data[\"text\"])\n",
    "\n",
    "                if hasattr(segment, \"words\") and segment.words:\n",
    "                    words.extend(self._process_words(segment.words))\n",
    "\n",
    "            return {\n",
    "                \"file\": audio_path.name,\n",
    "                \"info\": self._process_info(info),\n",
    "                \"segments\": segments,\n",
    "                \"words\": words,\n",
    "                \"full_text\": \" \".join(full_text),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            justsdk.print_error(f\"Transcription failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _process_segment(self, segment) -> Dict:\n",
    "        return {\n",
    "            \"id\": segment.id,\n",
    "            \"start\": segment.start,\n",
    "            \"end\": segment.end,\n",
    "            \"duration\": segment.end - segment.start,\n",
    "            \"text\": segment.text.strip(),\n",
    "            \"confidence\": round(np.exp(segment.avg_logprob), 4),\n",
    "            \"no_speech_prob\": segment.no_speech_prob,\n",
    "            \"start_time\": Utils.to_hms(segment.start),\n",
    "            \"end_time\": Utils.to_hms(segment.end),\n",
    "        }\n",
    "\n",
    "    def _process_words(self, words) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                \"word\": word.word,\n",
    "                \"start\": word.start,\n",
    "                \"end\": word.end,\n",
    "                \"probability\": word.probability,\n",
    "                \"start_time\": Utils.to_hms(word.start),\n",
    "                \"end_time\": Utils.to_hms(word.end),\n",
    "            }\n",
    "            for word in words\n",
    "        ]\n",
    "\n",
    "    def _process_info(self, info) -> Dict:\n",
    "        return {\n",
    "            \"language\": info.language,\n",
    "            \"language_probability\": info.language_probability,\n",
    "            \"duration\": info.duration,\n",
    "            \"duration_formatted\": Utils.to_hms(info.duration),\n",
    "        }\n",
    "\n",
    "\n",
    "transcriber = TranscriptionEngine(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Speaker Diarization Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiarizationEngine:\n",
    "    def __init__(self, hf_read_only_token: str):\n",
    "        self.pipeline = self._initialize_pipeline(hf_read_only_token)\n",
    "\n",
    "    def _initialize_pipeline(self, hf_read_only_token: str) -> Optional[Pipeline]:\n",
    "        try:\n",
    "            pipeline = Pipeline.from_pretrained(\n",
    "                \"pyannote/speaker-diarization-3.1\", use_auth_token=hf_read_only_token\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pipeline = pipeline.to(torch.device(\"cuda\"))\n",
    "                justsdk.print_info(\"Using GPU for diarization\")\n",
    "            else:\n",
    "                justsdk.print_info(\"Using CPU for diarization\")\n",
    "\n",
    "            return pipeline\n",
    "        except Exception as e:\n",
    "            justsdk.print_error(f\"Failed to initialize diarization pipeline: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process(self, audio_path: Path) -> Optional[Dict]:\n",
    "        if not self.pipeline:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            with ProgressHook() as hook:\n",
    "                diarization = self.pipeline(str(audio_path), hook=hook)\n",
    "\n",
    "            segments = []\n",
    "            speakers = set()\n",
    "\n",
    "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                segment_data = {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": turn.start,\n",
    "                    \"end\": turn.end,\n",
    "                    \"duration\": turn.end - turn.start,\n",
    "                    \"start_time\": Utils.to_hms(turn.start),\n",
    "                    \"end_time\": Utils.to_hms(turn.end),\n",
    "                }\n",
    "                segments.append(segment_data)\n",
    "                speakers.add(speaker)\n",
    "\n",
    "            speaker_stats = {speaker: 0.0 for speaker in speakers}\n",
    "            for segment in segments:\n",
    "                speaker_stats[segment[\"speaker\"]] += segment[\"duration\"]\n",
    "\n",
    "            return {\n",
    "                \"segments\": segments,\n",
    "                \"speakers\": sorted(speakers),\n",
    "                \"speaker_stats\": speaker_stats,\n",
    "                \"total_duration\": sum(speaker_stats.values()),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            justsdk.print_error(f\"Diarization failed: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "diarizer = DiarizationEngine(HF_READ_ONLY_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transcript Alignment and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_transcription_with_speakers(\n",
    "    transcription: Dict, diarization: Dict, threshold: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    aligned_segments = []\n",
    "\n",
    "    for trans_seg in transcription[\"segments\"]:\n",
    "        best_speaker = \"UNKNOWN\"\n",
    "        best_overlap = 0.0\n",
    "\n",
    "        for speaker_seg in diarization[\"segments\"]:\n",
    "            overlap_start = max(trans_seg[\"start\"], speaker_seg[\"start\"])\n",
    "            overlap_end = min(trans_seg[\"end\"], speaker_seg[\"end\"])\n",
    "            overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap_duration > 0:\n",
    "                overlap_ratio = overlap_duration / trans_seg[\"duration\"]\n",
    "                if overlap_ratio >= threshold and overlap_ratio > best_overlap:\n",
    "                    best_overlap = overlap_ratio\n",
    "                    best_speaker = speaker_seg[\"speaker\"]\n",
    "\n",
    "        aligned_segments.append(\n",
    "            {**trans_seg, \"speaker\": best_speaker, \"overlap_confidence\": best_overlap}\n",
    "        )\n",
    "\n",
    "    return aligned_segments\n",
    "\n",
    "\n",
    "def format_transcript(aligned_segments: List[Dict]) -> str:\n",
    "    lines = []\n",
    "    for segment in aligned_segments:\n",
    "        speaker_label = segment[\"speaker\"].replace(\"SPEAKER_\", \"Speaker \")\n",
    "        time_range = f\"[{segment['start_time']} - {segment['end_time']}]\"\n",
    "\n",
    "        line = f\"{time_range} {speaker_label}:\\n{segment['text']}\\n\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Audio Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"target_file\"] not in audio_files:\n",
    "    justsdk.print_error(f\"Target file '{CONFIG['target_file']}' not found\")\n",
    "    if audio_files:\n",
    "        CONFIG[\"target_file\"] = list(audio_files.keys())[0]\n",
    "        justsdk.print_info(f\"Using first available file: {CONFIG['target_file']}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No audio files found\")\n",
    "\n",
    "target_file = audio_files[CONFIG[\"target_file\"]][\"path\"]\n",
    "justsdk.print_info(f\"Processing: {target_file.name}\")\n",
    "\n",
    "display(Audio(target_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Starting transcription...\")\n",
    "transcription_result = transcriber.transcribe(target_file)\n",
    "\n",
    "if transcription_result:\n",
    "    info = transcription_result[\"info\"]\n",
    "    justsdk.print_success(\"Transcription completed\")\n",
    "    print(f\"Duration: {info['duration_formatted']}\")\n",
    "    print(\n",
    "        f\"Language: {info['language']} (confidence: {info['language_probability']:.3f})\"\n",
    "    )\n",
    "    print(f\"Segments: {len(transcription_result['segments'])}\")\n",
    "    print(f\"Words: {len(transcription_result['words'])}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Transcription failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Starting speaker diarization...\")\n",
    "diarization_result = diarizer.process(target_file)\n",
    "\n",
    "if diarization_result:\n",
    "    justsdk.print_success(\"Diarization completed\")\n",
    "    print(f\"Speakers found: {len(diarization_result['speakers'])}\")\n",
    "    print(f\"Speaker segments: {len(diarization_result['segments'])}\")\n",
    "\n",
    "    for speaker, duration in diarization_result[\"speaker_stats\"].items():\n",
    "        percentage = (duration / diarization_result[\"total_duration\"]) * 100\n",
    "        print(f\"  {speaker}: {Utils.to_hms(duration)} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    raise RuntimeError(\"Diarization failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Aligning transcription with speakers...\")\n",
    "aligned_segments = align_transcription_with_speakers(\n",
    "    transcription_result, diarization_result, CONFIG[\"diarization\"][\"overlap_threshold\"]\n",
    ")\n",
    "\n",
    "final_transcript = format_transcript(aligned_segments)\n",
    "justsdk.print_success(\"Alignment completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Final Speaker-Attributed Transcript:\")\n",
    "print(\"=\" * 80)\n",
    "print(final_transcript)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "speaker_colors = {\n",
    "    speaker: f\"C{i}\" for i, speaker in enumerate(diarization_result[\"speakers\"])\n",
    "}\n",
    "y_pos = 0\n",
    "\n",
    "for segment in diarization_result[\"segments\"]:\n",
    "    ax1.barh(\n",
    "        y_pos,\n",
    "        segment[\"duration\"],\n",
    "        left=segment[\"start\"],\n",
    "        color=speaker_colors[segment[\"speaker\"]],\n",
    "        alpha=0.7,\n",
    "        label=segment[\"speaker\"]\n",
    "        if segment[\"speaker\"] not in ax1.get_legend_handles_labels()[1]\n",
    "        else \"\",\n",
    "    )\n",
    "    y_pos += 1\n",
    "\n",
    "ax1.set_xlabel(\"Time (seconds)\")\n",
    "ax1.set_ylabel(\"Speaker Segments\")\n",
    "ax1.set_title(\"Speaker Diarization Timeline\")\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "speakers = list(diarization_result[\"speaker_stats\"].keys())\n",
    "durations = list(diarization_result[\"speaker_stats\"].values())\n",
    "colors = [speaker_colors[speaker] for speaker in speakers]\n",
    "\n",
    "ax2.pie(durations, labels=speakers, colors=colors, autopct=\"%1.1f%%\", startangle=90)\n",
    "ax2.set_title(\"Speaker Time Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_data = [seg[\"confidence\"] for seg in transcription_result[\"segments\"]]\n",
    "segment_durations = [seg[\"duration\"] for seg in transcription_result[\"segments\"]]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.hist(confidence_data, bins=20, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax1.axvline(\n",
    "    np.mean(confidence_data),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {np.mean(confidence_data):.3f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Confidence Score\")\n",
    "ax1.set_ylabel(\"Number of Segments\")\n",
    "ax1.set_title(\"Transcription Confidence Distribution\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.scatter(segment_durations, confidence_data, alpha=0.6, color=\"coral\")\n",
    "ax2.set_xlabel(\"Segment Duration (seconds)\")\n",
    "ax2.set_ylabel(\"Confidence Score\")\n",
    "ax2.set_title(\"Confidence vs Segment Duration\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confidence Statistics:\")\n",
    "print(f\"  Mean: {np.mean(confidence_data):.3f}\")\n",
    "print(f\"  Median: {np.median(confidence_data):.3f}\")\n",
    "print(f\"  Min: {np.min(confidence_data):.3f}\")\n",
    "print(f\"  Max: {np.max(confidence_data):.3f}\")\n",
    "print(f\"  Std Dev: {np.std(confidence_data):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(aligned_segments)\n",
    "print(\"Aligned Segments Summary:\")\n",
    "print(\n",
    "    summary_df[\n",
    "        [\n",
    "            \"speaker\",\n",
    "            \"start_time\",\n",
    "            \"end_time\",\n",
    "            \"confidence\",\n",
    "            \"overlap_confidence\",\n",
    "            \"text\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
