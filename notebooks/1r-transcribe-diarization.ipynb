{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio transcription + Speaker diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transcribe audio using `faster-whisper`.\n",
    "\n",
    "2. Speaker diarization using `pyannote-audio`.\n",
    "\n",
    "3. Combine the results from both steps to create a structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import justsdk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from faster_whisper import WhisperModel\n",
    "from IPython.display import Audio, display\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import Optional\n",
    "from _constants import HF_TOKEN, SAMPLE_DIR, MODEL_DIR\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TARGET_SAMPLE = \"project-proposal.mp3\"\n",
    "\n",
    "WHISPER_MODEL = \"base\"\n",
    "\n",
    "WHISPER_CONFIG = {\n",
    "    \"model_size\": WHISPER_MODEL,\n",
    "    \"device\": \"cpu\",\n",
    "    \"compute_type\": \"int8\",\n",
    "    \"num_workers\": 2,\n",
    "    \"download_root\": str(MODEL_DIR / f\"whisper-{WHISPER_MODEL}\"),\n",
    "}\n",
    "\n",
    "AUDIO_CONFIG = {\n",
    "    \"language\": \"en\",\n",
    "    \"task\": \"transcribe\",\n",
    "    \"beam_size\": 5,  # Paths searches during decoding\n",
    "    \"best_of\": 5,\n",
    "    \"patience\": 1,\n",
    "    \"length_penalty\": 1,\n",
    "    \"temperature\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  # Temperature fallback\n",
    "    \"compression_ratio_threshold\": 2.4,  # Reject if text is too repetitive\n",
    "    \"log_prob_threshold\": -1.0,  # Threshold for confidence levels\n",
    "    \"no_speech_threshold\": 0.6,  # Threshold for non-speech detection\n",
    "    \"word_timestamps\": True,  # Generate word-level timestamps\n",
    "    \"vad_filter\": True,  # Skip silent parts\n",
    "    \"vad_parameters\": {\n",
    "        \"threshold\": 0.5,\n",
    "        \"min_speech_duration_ms\": 250,\n",
    "        \"max_speech_duration_s\": float(\"inf\"),\n",
    "        \"min_silence_duration_ms\": 2000,\n",
    "        \"speech_pad_ms\": 400,\n",
    "    },\n",
    "}\n",
    "\n",
    "DIARIZATION_CONFIG = {\n",
    "    \"num_speakers\": None,\n",
    "    \"min_speakers\": None,\n",
    "    \"max_speakers\": None,\n",
    "    \"segmentation_onset\": 0.5,  # XXX: Learn more about this threshold\n",
    "    \"clustering\": {  # XXX: Learn more about clustering options\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15,\n",
    "        \"threshold\": 0.7,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Helper: General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to `HH:MM:SS.mm` format.\"\"\"\n",
    "    if seconds < 0:\n",
    "        return \"00:00:00.00\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    hours = int(td.total_seconds() // 3600)\n",
    "    minutes = int((td.total_seconds() % 3600) // 60)\n",
    "    seconds = td.total_seconds() % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:05.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_files() -> dict:\n",
    "    sample_ext = [\".mp3\", \".mp4\"]\n",
    "    sample_dirs = [\"audio\", \"video\"]\n",
    "\n",
    "    samples = [\n",
    "        sample\n",
    "        for sample_dir in sample_dirs\n",
    "        for ext in sample_ext\n",
    "        for sample in (SAMPLE_DIR / sample_dir).glob(f\"*{ext}\")\n",
    "    ]\n",
    "\n",
    "    samples_dict = {}\n",
    "    for sample in samples:\n",
    "        size_mb = sample.stat().st_size / (1024 * 1024)\n",
    "        samples_dict[sample.name] = {\n",
    "            \"path\": sample,\n",
    "            \"size_mb\": size_mb,\n",
    "        }\n",
    "\n",
    "    return samples_dict\n",
    "\n",
    "\n",
    "sample_input = get_sample_files()\n",
    "justsdk.print_info(\"Sample files found:\")\n",
    "for name, info in sample_input.items():\n",
    "    print(f\"  {name} ({info['size_mb']:.2f} MB): {info['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Init `faster-whisper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    whisper_model = WhisperModel(\n",
    "        model_size_or_path=WHISPER_CONFIG[\"model_size\"],\n",
    "        device=WHISPER_CONFIG[\"device\"],\n",
    "        compute_type=WHISPER_CONFIG[\"compute_type\"],\n",
    "        num_workers=WHISPER_CONFIG[\"num_workers\"],\n",
    "        download_root=WHISPER_CONFIG[\"download_root\"],\n",
    "    )\n",
    "    justsdk.print_success(\"Loaded Whisper model.\")\n",
    "except Exception as e:\n",
    "    justsdk.print_error(f\"Failed to load Whisper model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Helper: Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path: Path, model: WhisperModel, config: dict) -> dict:\n",
    "    try:\n",
    "        segments_generator, info = model.transcribe(str(audio_path), **config)\n",
    "        segments = list(segments_generator)\n",
    "\n",
    "        processed_segments = []\n",
    "        word_segments = []\n",
    "        full_text_parts = []\n",
    "\n",
    "        for segment in segments:\n",
    "            seg_dict = _process_segment(segment)\n",
    "            processed_segments.append(seg_dict)\n",
    "            full_text_parts.append(seg_dict[\"text\"])\n",
    "\n",
    "            # Process words if available\n",
    "            if hasattr(segment, \"words\") and segment.words:\n",
    "                word_segments.extend(_process_words(segment.words))\n",
    "\n",
    "        return {\n",
    "            \"file\": audio_path.name,\n",
    "            \"audio_info\": _process_audio_info(info),\n",
    "            \"num_segments\": len(processed_segments),\n",
    "            \"segments\": processed_segments,\n",
    "            \"num_words\": len(word_segments),\n",
    "            \"words\": word_segments,\n",
    "            \"full_text\": \" \".join(full_text_parts),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        justsdk.print_error(f\"Failed to transcribe {audio_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _process_segment(segment) -> dict:\n",
    "    return {\n",
    "        \"id\": segment.id,\n",
    "        \"start\": segment.start,\n",
    "        \"end\": segment.end,\n",
    "        \"text\": segment.text.strip(),\n",
    "        \"tokens\": segment.tokens,\n",
    "        \"temperature\": segment.temperature,\n",
    "        \"avg_logprob\": segment.avg_logprob,\n",
    "        \"no_speech_prob\": segment.no_speech_prob,\n",
    "        \"compression_ratio\": segment.compression_ratio,\n",
    "        \"start_formatted\": format_timestamp(segment.start),\n",
    "        \"end_formatted\": format_timestamp(segment.end),\n",
    "        \"duration\": segment.end - segment.start,\n",
    "    }\n",
    "\n",
    "\n",
    "def _process_words(words) -> list:\n",
    "    return [\n",
    "        {\n",
    "            \"word\": word.word,\n",
    "            \"start\": word.start,\n",
    "            \"end\": word.end,\n",
    "            \"probability\": word.probability,\n",
    "            \"start_formatted\": format_timestamp(word.start),\n",
    "            \"end_formatted\": format_timestamp(word.end),\n",
    "        }\n",
    "        for word in words\n",
    "    ]\n",
    "\n",
    "\n",
    "def _process_audio_info(info) -> dict:\n",
    "    return {\n",
    "        \"language\": info.language,\n",
    "        \"language_probability\": info.language_probability,\n",
    "        \"duration\": info.duration,\n",
    "        \"duration_formatted\": format_timestamp(info.duration),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Transcribing audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_input:\n",
    "    sample_selected = sample_input[TARGET_SAMPLE][\"path\"]\n",
    "    justsdk.print_info(f\"Selected sample: {sample_selected.name} â€” {sample_selected}\")\n",
    "    display(Audio(sample_selected))\n",
    "    transcription = transcribe_audio(\n",
    "        audio_path=sample_selected,\n",
    "        model=whisper_model,\n",
    "        config=AUDIO_CONFIG,\n",
    "    )\n",
    "else:\n",
    "    justsdk.print_error(\"No sample files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Check full transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcription:\n",
    "    justsdk.print_success(f\"Completed transcription for {transcription['file']}\")\n",
    "    print(f\"\"\"\n",
    "    Duration: \\t{transcription[\"audio_info\"][\"duration_formatted\"]}\n",
    "    No. of segments: \\t{transcription[\"num_segments\"]}\n",
    "    No. of words: \\t{transcription[\"num_words\"]}\n",
    "    \"\"\")\n",
    "    full_text = transcription[\"full_text\"].replace(\". \", \".\\n  \")\n",
    "    justsdk.print_info(\"Full Transcription:\")\n",
    "    print(f\"  {full_text}\")\n",
    "else:\n",
    "    justsdk.print_error(\"Transcription failed or returned no results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Check segments from transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcription and transcription[\"segments\"]:\n",
    "    segments_df = pd.DataFrame(transcription[\"segments\"])\n",
    "    for index, seg in segments_df.iterrows():\n",
    "        confidence = round(np.exp(seg[\"avg_logprob\"]), 4)\n",
    "        print(f\"\"\"\n",
    "        [{seg[\"start_formatted\"]} -> {seg[\"end_formatted\"]}] ({seg[\"duration\"]:.2f}s)\n",
    "          {seg[\"text\"]}\n",
    "\n",
    "          Confidence: {confidence}\n",
    "          Non-speech prob.: {seg[\"no_speech_prob\"]:.4f}\n",
    "        \"\"\")\n",
    "else:\n",
    "    justsdk.print_error(\"No segments available for display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Metrics: Transcription segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Helper: Speaker diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_diarization_pipeline(auth_token: Optional[str] = None) -> Pipeline:\n",
    "    try:\n",
    "        if not auth_token:\n",
    "            raise ValueError(\"Hugging Face auth token is required for diarization.\")\n",
    "\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=auth_token\n",
    "        )\n",
    "\n",
    "        # Use GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline = pipeline.to(torch.device(\"cuda\"))\n",
    "            justsdk.print_info(\"Using GPU for speaker diarization\")\n",
    "        else:\n",
    "            justsdk.print_info(\"Using CPU for speaker diarization\")\n",
    "\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        justsdk.print_error(f\"Failed to initialize diarization pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def perform_speaker_diarization(\n",
    "    audio_path: Path, pipeline: Pipeline, config: dict = DIARIZATION_CONFIG\n",
    ") -> dict:\n",
    "    print(audio_path)\n",
    "    try:\n",
    "        with ProgressHook() as hook:\n",
    "            diarization = pipeline(\n",
    "                str(audio_path),\n",
    "                hook=hook,\n",
    "                num_speakers=config.get(\"num_speakers\"),\n",
    "                min_speakers=config.get(\"min_speakers\"),\n",
    "                max_speakers=config.get(\"max_speakers\"),\n",
    "            )\n",
    "        speaker_segments: list = []\n",
    "        speakers: set = set()\n",
    "\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            speaker_segments.append(\n",
    "                {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": turn.start,\n",
    "                    \"end\": turn.end,\n",
    "                    \"start_formatted\": format_timestamp(turn.start),\n",
    "                    \"end_formatted\": format_timestamp(turn.end),\n",
    "                    \"duration\": turn.end - turn.start,\n",
    "                }\n",
    "            )\n",
    "            speakers.add(speaker)\n",
    "\n",
    "        speaker_stats: dict = {speaker: 0.0 for speaker in speakers}\n",
    "        for segment in speaker_segments:\n",
    "            speaker_stats[segment[\"speaker\"]] += segment[\"duration\"]\n",
    "\n",
    "        return {\n",
    "            \"segments\": speaker_segments,\n",
    "            \"num_segments\": len(speaker_segments),\n",
    "            \"speakers\": sorted(list(speakers)),\n",
    "            \"num_speakers\": len(speakers),\n",
    "            \"speaker_stats\": speaker_stats,\n",
    "            \"total_duration\": sum(speaker_stats.values()),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        justsdk.print_error(f\"Failed to perform speaker diarization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def align_transcription_with_speakers(\n",
    "    transcription: dict, diarization_res: dict, overlap_threshold: float = 0.5\n",
    ") -> list:\n",
    "    aligned_segments: list = []\n",
    "\n",
    "    for trans_seg in transcription[\"segments\"]:\n",
    "        trans_start = trans_seg[\"start\"]\n",
    "        trans_end = trans_seg[\"end\"]\n",
    "        trans_duration = trans_end - trans_start\n",
    "\n",
    "        speaker_overlaps: list = []\n",
    "\n",
    "        for speaker_seg in diarization_res[\"segments\"]:\n",
    "            speaker_start = speaker_seg[\"start\"]\n",
    "            speaker_end = speaker_seg[\"end\"]\n",
    "\n",
    "            overlap_start = max(trans_start, speaker_start)\n",
    "            overlap_end = min(trans_end, speaker_end)\n",
    "            overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap_duration > 0:\n",
    "                overlap_ratio = overlap_duration / trans_duration\n",
    "                if overlap_ratio >= overlap_threshold:\n",
    "                    speaker_overlaps.append(\n",
    "                        {\n",
    "                            \"speaker\": speaker_seg[\"speaker\"],\n",
    "                            \"overlap_ratio\": overlap_ratio,\n",
    "                            \"overlap_duration\": overlap_duration,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if speaker_overlaps:\n",
    "            best_speaker = max(speaker_overlaps, key=lambda x: x[\"overlap_ratio\"])[\n",
    "                \"speaker\"\n",
    "            ]\n",
    "            formatted_best_speaker = best_speaker.replace(\n",
    "                \"SPEAKER_\", f\"{justsdk.Fore.MAGENTA}SPEAKER_{justsdk.Fore.RESET}\"\n",
    "            )\n",
    "            speaker = formatted_best_speaker\n",
    "        else:\n",
    "            speaker = f\"{justsdk.Fore.RED}UNKNOWN{justsdk.Fore.RESET}\"\n",
    "\n",
    "        aligned_segments.append(\n",
    "            {\n",
    "                **trans_seg,\n",
    "                \"speaker\": speaker,\n",
    "            }\n",
    "        )\n",
    "    return aligned_segments\n",
    "\n",
    "\n",
    "def format_diarized_transcript(aligned_segments: list) -> str:\n",
    "    formatted_lines: list = []\n",
    "    for seg in aligned_segments:\n",
    "        line = (\n",
    "            f\"  [{seg['start_formatted']} - {seg['end_formatted']}] {seg['speaker']}:\\n\"\n",
    "            f\"  {seg['text']}\\n\"\n",
    "        )\n",
    "        formatted_lines.append(line)\n",
    "    return \"\\n\".join(formatted_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Speaker diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justsdk.print_info(\"Initializing diarization pipeline...\")\n",
    "diarization_pipeline = initialize_diarization_pipeline(HF_TOKEN)\n",
    "\n",
    "if diarization_pipeline and transcription:\n",
    "    justsdk.print_info(f\"Performing speaker diarization on {sample_selected.name}\")\n",
    "\n",
    "    diarization_result = perform_speaker_diarization(\n",
    "        audio_path=sample_selected, pipeline=diarization_pipeline\n",
    "    )\n",
    "\n",
    "    if diarization_result:\n",
    "        justsdk.print_success(\n",
    "            f\"Diarization completed. Found {diarization_result['num_speakers']} speakers.\"\n",
    "        )\n",
    "    else:\n",
    "        justsdk.print_error(\"Diarization failed or returned no results.\")\n",
    "\n",
    "else:\n",
    "    justsdk.print_error(\n",
    "        \"Diarization pipeline initialization failed or transcription is missing.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Analyzing diarization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if diarization_result:\n",
    "    aligned_segments = align_transcription_with_speakers(\n",
    "        transcription, diarization_result, overlap_threshold=0.5\n",
    "    )\n",
    "    formatted_transcript = format_diarized_transcript(aligned_segments)\n",
    "    justsdk.print_info(\"Formatted Diarized Transcript:\")\n",
    "    print(formatted_transcript)\n",
    "else:\n",
    "    justsdk.print_error(\"No diarization results available for alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Metrics: Speaker diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize diarization results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
